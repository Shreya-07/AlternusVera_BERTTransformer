{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreya-07/AlternusVera_BERTTransformer/blob/main/BERT_Transformer_Shreya_Nimbhorkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uqSCsvj4u2Y",
        "outputId": "7bc966ab-0879-4202-b0c8-8217287c98ab"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from scipy import sparse\n",
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5HzNt9-V5c_f"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Thetrio_bert():\n",
        "\n",
        "    def cleaning(raw_data):\n",
        "    \n",
        "      # 1. Remove non-letters/Special Characters and Punctuations\n",
        "      data = re.sub(\"[^a-zA-Z]\", \" \", str(raw_data))\n",
        "      \n",
        "      # 2. Convert to lower case.\n",
        "      data =  data.lower()\n",
        "      \n",
        "      # 3. Tokenize.\n",
        "      data_words = nltk.word_tokenize(data)\n",
        "      \n",
        "      # 4. Convert the stopwords list to \"set\" data type.\n",
        "      stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "      \n",
        "      # 5. Remove stop words. \n",
        "      words = [w for w in  data_words  if not w in stops]\n",
        "      \n",
        "      # 6. Lemmentize \n",
        "      wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "      \n",
        "      # 7. Stemming\n",
        "      stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "      \n",
        "      # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "      return \" \".join(wordnet_lem)\n",
        "\n",
        "    def scaleTruth(mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):  \n",
        "\n",
        "      max_len = 300\n",
        "      cols = ['mostly_true_count','half_true_count','barely_true_count','false_count','pants_on_fire_count']\n",
        "      history_features = pd.DataFrame(np.array([[mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count]]),columns=cols)\n",
        "                             \n",
        "      scaler = StandardScaler()\n",
        "      scaler.fit(history_features)\n",
        "      StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "      history_features = scaler.transform(history_features)\n",
        "      num_rows, num_cols = history_features.shape\n",
        "      suffix = np.zeros([num_rows, max_len-num_cols])\n",
        "      history_features = np.concatenate((history_features,suffix),axis=1)\n",
        "      return history_features\n",
        "\n",
        "    def tokenize_text(text,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "      textline = []\n",
        "      # Load the BERT tokenizer.\n",
        "      X_t = Thetrio_bert.scaleTruth(mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "      input_ids = []\n",
        "      attention_masks = []\n",
        "      textline.append(text)\n",
        "      for sent in textline:\n",
        "\n",
        "          encoded_dict = tokenizer.encode_plus(\n",
        "                              sent,                      # Sentence to encode.\n",
        "                              add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                              max_length = 300,      # Pad & truncate all sentences.\n",
        "                              pad_to_max_length = True,\n",
        "                              return_attention_mask = True,   # Construct attn. masks.\n",
        "                              return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                      )\n",
        "          \n",
        "          # Add the encoded sentence to the list.    \n",
        "          input_ids.append(encoded_dict['input_ids'])\n",
        "          \n",
        "          \n",
        "          # And its attention mask (simply differentiates padding from non-padding).\n",
        "          attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "      # Convert the lists into tensors.\n",
        "      input_ids = torch.cat(input_ids, dim=0)\n",
        "      attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "      input_ids = input_ids.numpy().astype(float)\n",
        "      for j in range(0,299):\n",
        "        if(input_ids[0][j]==102):\n",
        "          input_ids[0][j] = X_t[0][0]\n",
        "          input_ids[0][j+1] = X_t[0][1]\n",
        "          input_ids[0][j+2] = X_t[0][2]\n",
        "          input_ids[0][j+3] = X_t[0][3]\n",
        "          input_ids[0][j+4] = X_t[0][4]\n",
        "          input_ids[0][j+5] = 102\n",
        "          break\n",
        "              \n",
        "      input_ids = torch.from_numpy(input_ids) \n",
        "\n",
        "      (test_input_ids,  \n",
        "      test_attention_masks) = (input_ids, attention_masks)\n",
        "      return test_input_ids, test_attention_masks\n",
        "\n",
        "  \n",
        "    def get_bert_predictions(text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "\n",
        "      #Adding some latent variables from the data\n",
        "      text     =  text+ source + context\n",
        "      text     = Thetrio_bert.cleaning(text)\n",
        "      b_input_ids, b_input_mask = Thetrio_bert.tokenize_text(text,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "      \n",
        "      #storing values to GPU\n",
        "      b_input_ids.cuda()\n",
        "      b_input_mask.cuda()\n",
        "\n",
        "      model = BertForSequenceClassification.from_pretrained(\"/content/\")\n",
        "\n",
        "      # model.cuda()\n",
        "      desc = model.cuda()\n",
        "      \n",
        "      # Put model in evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Tracking variables \n",
        "      predictions = []\n",
        "      \n",
        "      with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "          b_input_ids = torch.tensor(b_input_ids).cuda().long()\n",
        "          b_input_mask = torch.tensor(b_input_mask).cuda().long()\n",
        "          outputs = model(b_input_ids, token_type_ids=None, \n",
        "                              attention_mask=b_input_mask)\n",
        "      \n",
        "      logits = outputs[0]\n",
        "      \n",
        "      # Move logits to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "   \n",
        "      # Store predictions  \n",
        "      predictions.append(logits)\n",
        "      \n",
        "      # Combine the results across the batches.\n",
        "      predictions = np.concatenate(predictions, axis=0)\n",
        "      tensorProbability = F.softmax(torch.tensor(predictions))\n",
        "      # Take the highest scoring output as the predicted label.\n",
        "      predicted_labels = np.argmax(predictions, axis=1)\n",
        "      tensorProbability = tensorProbability.numpy()\n",
        "      predicted = str(predicted_labels[0])\n",
        "      fakeness_probability = tensorProbability[0][3] + tensorProbability[0][4] + tensorProbability[0][5]\n",
        "      fakeness_probability = fakeness_probability / 3\n",
        "      return int(predicted), fakeness_probability"
      ],
      "metadata": {
        "id": "jXGIT5--5Zcy"
      },
      "execution_count": 71,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT_Transformer_Shreya_Nimbhorkar.ipynb",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "0m5N7Z8O5ZBO"
      ],
      "authorship_tag": "ABX9TyNGGiex9DYU5wPQICzpkm/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}